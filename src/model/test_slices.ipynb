{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "from test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'func_jsonal'\n",
    "train_batch_size = 256\n",
    "test_batch_size = 16\n",
    "idx_key = 'slice_id'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args()\n",
    "args.level = 'slice'\n",
    "args.train_batch_size = train_batch_size\n",
    "args.eval_batch_size = test_batch_size\n",
    "args.output_dir = os.path.join(BASE_DIR, 'saved_models', 'func_jsonal', str(train_batch_size))\n",
    "args.test_data_file = os.path.join(BASE_DIR, 'processed_dataset', dataset, 'test.jsonl')\n",
    "args.idx_key = idx_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_data(path):\n",
    "    f = open(path, 'r')\n",
    "    data = f.read().split(\"\\n\")\n",
    "    f.close()\n",
    "    return [json.loads(sample) for sample in data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = read_data(args.test_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data = test_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "final_test_data = []\n",
    "cnt = 0\n",
    "for func in temp_data:\n",
    "    for slice in func['slices']:\n",
    "        sample = {}\n",
    "        sample['ind'] = cnt\n",
    "        sample['func'] = slice\n",
    "        sample['id'] = func['id']\n",
    "        sample['target'] = func['target']\n",
    "        final_test_data.append(sample)\n",
    "        cnt += 1\n",
    "print(len(final_test_data))\n",
    "args.idx_key = 'ind'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_data = final_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running Test *****\n",
      "  Num examples = %d 22\n",
      "  Batch size = %d 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:11<00:00,  5.88s/it]\n"
     ]
    }
   ],
   "source": [
    "args.data = top_k_data\n",
    "predictions = predict(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [slice['target'] for slice in top_k_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_matrices(labels, predictions):\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions)\n",
    "    prec = precision_score(labels, predictions)\n",
    "    recall = recall_score(labels, predictions)\n",
    "    print(\"Accuracy \", acc, 'F1 Score: ', f1, 'Precision: ', prec, 'Recall: ', recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_pred = {}\n",
    "id_to_lab = {}\n",
    "for indx, sample in enumerate(top_k_data):\n",
    "\n",
    "    id_to_pred[sample['id']] = max(id_to_pred.get(sample['id'], 0), predictions[indx])\n",
    "    id_to_lab[sample['id']] = sample['target']    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "labs = []\n",
    "preds = []\n",
    "for id in id_to_pred:\n",
    "    preds.append(id_to_pred[id])\n",
    "    labs.append(id_to_lab[id])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy  0.8 F1 Score:  0.8571428571428571 Precision:  0.8571428571428571 Recall:  0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "print_matrices(labs, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('codebert')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "23f55371fe6a3bc989ef5682b29e7895456b08d287f3d8049f420667cb23d85a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
